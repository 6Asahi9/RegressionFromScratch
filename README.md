### Regression From Scratch
Building Linear Regression from the ground up—no shortcuts, no libraries (except NumPy for basic operations).

### About This Project
This project implements Linear Regression from scratch, following a structured approach:

✔️ Gradient Descent (GD) – Basic optimization

✔️ Stochastic Gradient Descent (SGD) – Faster updates

✔️ Momentum – Smoother convergence

✔️ Adam Optimizer – The ultimate learning rate optimizer

The goal is to understand regression models at the core while experimenting with different optimization techniques.
